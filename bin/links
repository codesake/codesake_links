#!/usr/bin/env ruby
require "rainbow"
require 'getoptlong'

require 'anemone'
require "codesake-commons"
require "links"

require 'data_mapper'
require 'dm-sqlite-adapter'

class Scan
  include DataMapper::Resource

  property :id,           Serial
  property :base,         String, :length=>256, :required => true
  property :tool,         String
  property :version,      String
  property :created_at,   DateTime, :default=>DateTime.now
  property :updated_at,   DateTime, :default=>DateTime.now

end

class Link
  include DataMapper::Resource

  property :id,           Serial
  property :path,         String, :length=>256, :required => true
  property :q,            String, :length=>256
  property :tested,       Boolean, :default=>false
  property :created_at,   DateTime, :default=>DateTime.now
  property :updated_at,   DateTime, :default=>DateTime.now

  def self.all_dynamic
    Link.all(:q.not => nil)
  end
end

POST_WITHOUT_SLASH  = %r[\d{4}\/\d{2}\/[^\/]+$]
POST_WITH_SLASH     = %r[\d{4}\/\d{2}\/[\w-]+\/$]
ANY_POST            = Regexp.union POST_WITHOUT_SLASH, POST_WITH_SLASH
ANY_PAGE            = %r[page\/\d+]
ANY_PATTERN         = Regexp.union ANY_PAGE, ANY_POST

APPNAME = File.basename($0)

logger = Codesake::Commons::Logging.instance
logger.toggle_syslog

opts = GetoptLong.new(
  [ '--help', '-h', GetoptLong::NO_ARGUMENT ],
  [ '--version', '-v', GetoptLong::NO_ARGUMENT ],
  [ '--dynamic', '-d', GetoptLong::NO_ARGUMENT ],
  [ '--bulk', '-b', GetoptLong::REQUIRED_ARGUMENT ],
  [ '--proxy', '-P',  GetoptLong::REQUIRED_ARGUMENT ],
  [ '--crawl', '-c', GetoptLong::NO_ARGUMENT ],
  [ '--robots', '-r', GetoptLong::NO_ARGUMENT ]
)
trap("INT") { logger.die("[INTERRUPTED]") }


list      = []
robots    = false
bulk      = false
show_code = false
crawl     = false
dynamic   = false
proxy     = {:host=>nil, :port=>-1}

opts.each do |opt, arg|
  case opt
  when '--help'
    puts "usage: links [-bvh] [filename]"
    puts "   -b filename: loads the url list from a plain text file"
    puts "   -r : parse robots.txt and make requests to disallowed urls"
    puts "   -c : shows the return code instead of human readable answer"
    puts "   -P host:port : connect using a proxy server. Useful in combination with Paros, Owasp Zap and other"
    puts "   -v : shows version information"
    puts "   -h : shows this help"
    exit 0
  when '--dynamic'
    dynamic = true
  when '--version'
    puts "#{Codesake::Links::VERSION}"
    exit 0
  when '--crawl'
    crawl= true
  when '--proxy'
    proxy[:host]=arg.split(':')[0]
    proxy[:port]=arg.split(':')[1].to_i
  when '--robots'
    robots=true
  when '--bulk'
    bulk=true
    if ! File.exists?(arg)
      puts "links: file not found (#{arg})".color(:red)
      exit 1
    end
    list = File.open(arg).readlines
    if list.count <= 0
      puts "links: invalid url list".color(:red)
      exit 1
    end
  end
end

target = ARGV.shift
logger.helo APPNAME, Codesake::Links::VERSION

db_name = URI.parse(target).host.gsub('.','_')
DataMapper.setup(:default, "sqlite3://#{File.join(Dir.pwd, db_name)}.db")
DataMapper.finalize
DataMapper.auto_upgrade!

# list<<target if list.empty?

logger.die("missing target") if target.nil?
# logger.die("no -b or -r option specified") unless bulk or robots

if robots
  res = Codesake::Links::Api.robots(target)
  list = res[:disallow_list]
  logger.err "#{target}: no robots.txt found (#{res[:error]})\n" if res[:status] == :KO
  logger.ok "no disallowed entries to test on #{target}" if list.empty?
  logger.ok "found #{list.size} disallowed url(s) on #{target}" unless list.empty?
  list.each do |l|
    logger.ok "#{l} - #{Codesake::Links::Api.code(target+l, nil)}"
  end
  logger.bye
  Kernel.exit(0)
end

if bulk

  list.each do |l|
    unless l.start_with? "#"

      l = l.chomp if l.end_with? "\n"
      l = '/'+l unless l.start_with? '/'

      url = target + l
      start = Time.now
      code = Codesake::Links::Api.code(url, nil)
      stop = Time.now

      str=Codesake::Links::Api.human(code)

      if code == "200"
        Codesake::Links::Utils.print_str(url, logger, str, start, stop)     unless show_code 
        Codesake::Links::Utils.print_code(url, logger, code, start, stop)   if show_code 
      end


      if code == 301 or code == 302
        start = Time.now
        new_link = Codesake::Links::Api.follow(l, proxy)
        stop = Time.now
        logger.log "following from #{l} to #{new_link}\n"
        str=Codesake::Links::Api.human(code)

        Codesake::Links::Utils.print_str(logger, str, start, stop)    unless show_code
        Codesake::Links::Utils.print_code(logger, code, start, stop)  if show_code

      end
    end

  end
end

if dynamic
  list = Link.all_dynamic
  logger.log "#{list.size} dynamic urls found during last crawl"
  list.each do |l|
    logger.ok "#{l.path}/#{l.q}"
  end
end

if crawl
  s=Scan.first(:base=>target)
  unless s.nil?
    s=Scan.new
    s.base=target
    s.tool=APPNAME
    s.version = VERSION
    s.save
  end

  logger.log "start crawling #{target}"

  Anemone.crawl(target, :redirect_limit=>2, :depth_limit => 5) do |anemone|
    anemone.on_every_page do |page|
      l = Link.first(:path=>page.url.path)
      if l.nil?
        l = Link.new
        l.path = page.url.path
        l.q = page.url.query
        saved = l.save
        logger.ok "adding #{page.url.path}" if saved
        logger.err "error saving #{page.url.path}: #{l.errors.inspect}" unless saved
      else
        logger.warn "skipping #{page.url.path}"
      end
    end
  end
end

# Anemone.crawl(target) do |anemone|
#   anemone.focus_crawl do |page|
#     page.links.keep_if { |link| link.to_s.match(ANY_PATTERN) } # crawl only links that are pages or blog posts
#   end
#   anemone.on_pages_like(POST_WITH_SLASH) do |page|
#     title = page.doc.at_xpath("//div[@role='main']/header/h1").text rescue nil
#     tag = page.doc.at_xpath("//header/div[@class='post-data']/p/a").text rescue nil

#     if title and tag
#       post = {title: title, tag: tag}
#       logger.log "Inserting #{post.inspect}"
#     end
#   end
# end
